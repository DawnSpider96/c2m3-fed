{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DawnSpider96/L361-Federated-Learning/blob/release/Copy_of_L361_2025_Lab_1_From_Centralised_To_Federated_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1P9epXi-IZ0"
      },
      "source": [
        "## Dependencies\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fTM8comw-IZ2"
      },
      "outputs": [],
      "source": [
        "# `pip` could produce some errors. Do not worry about them.\n",
        "# The execution has been verified; it's working anyway.\n",
        "# ! pip install --quiet --upgrade \"pip\"\n",
        "# ! pip install --quiet matplotlib tqdm seaborn\n",
        "# ! pip install git+https://github.com/Iacob-Alexandru-Andrei/flower.git@teaching \\\n",
        "#     torch torchvision ray==\"2.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zl2hUa1-IZ2"
      },
      "source": [
        "### Imports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sys\n",
        "# sys.path.append('../../')\n",
        "# print(sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z7xM_SyD-IZ4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dawn/venvs/fed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-03-27 21:14:03.423900: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-03-27 21:14:03.473314: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-03-27 21:14:03.474759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-03-27 21:14:04.366369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2025-03-27 21:14:05,605\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "from typing import Any\n",
        "from logging import INFO\n",
        "from collections import defaultdict, OrderedDict\n",
        "from collections.abc import Sequence, Callable\n",
        "import numbers\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from enum import IntEnum\n",
        "import flwr\n",
        "from flwr.server import History, ServerConfig\n",
        "from flwr.server.strategy import FedAvgM as FedAvg, Strategy\n",
        "from c2m3.flower.fed_frank_wolfe_strategy import FrankWolfeSync\n",
        "from flwr.common import log, NDArrays, Scalar, Parameters, ndarrays_to_parameters\n",
        "from flwr.client.client import Client\n",
        "\n",
        "from c2m3.common.client_utils import (\n",
        "    Net,\n",
        "    load_femnist_dataset,\n",
        "    get_network_generator_cnn as get_network_generator,\n",
        "    train_femnist,\n",
        "    test_femnist,\n",
        "    save_history,\n",
        "    get_model_parameters,\n",
        "    set_model_parameters\n",
        ")\n",
        "\n",
        "\n",
        "# Add new seeds here for easy autocomplete\n",
        "class Seeds(IntEnum):\n",
        "    \"\"\"Seeds for reproducibility.\"\"\"\n",
        "\n",
        "    DEFAULT = 42 # [42, 123, 456, 789, 101]\n",
        "\n",
        "\n",
        "np.random.seed(Seeds.DEFAULT)\n",
        "random.seed(Seeds.DEFAULT)\n",
        "torch.manual_seed(Seeds.DEFAULT)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "PathType = Path | str | None\n",
        "\n",
        "\n",
        "def get_device() -> str:\n",
        "    \"\"\"Get the device (cuda, mps, cpu).\"\"\"\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "        device = \"mps\"\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "341p1c5o-IZ4"
      },
      "outputs": [],
      "source": [
        "home_dir = Path.cwd() / \"..\"\n",
        "dataset_dir: Path = home_dir / \"data\" / \"femnist\"\n",
        "data_dir: Path = dataset_dir / \"data\"\n",
        "centralized_partition: Path = dataset_dir / \"client_data_mappings\" / \"centralized\"\n",
        "centralized_mapping: Path = dataset_dir / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
        "federated_partition: Path = dataset_dir / \"client_data_mappings\" / \"fed_natural\"\n",
        "\n",
        "# Decompress dataset\n",
        "# if not dataset_dir.exists():\n",
        "#     with tarfile.open(home_dir / \"femnist.tar.gz\", \"r:gz\") as tar:\n",
        "#         tar.extractall(path=home_dir)\n",
        "#     log(INFO, \"Dataset extracted in %s\", dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Q6qaJbjac5"
      },
      "source": [
        "## Build Flower FL client.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SCqhfpnJobOZ"
      },
      "outputs": [],
      "source": [
        "class FlowerRayClient(flwr.client.NumPyClient):\n",
        "    \"\"\"Flower client for the FEMNIST dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        partition_dir: Path,\n",
        "        model_generator: Callable[[], Module],\n",
        "    ) -> None:\n",
        "        \"\"\"Init the client with its unique id and the folder to load data from.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            cid (int): Unique client id for a client used to map it to its data\n",
        "                partition\n",
        "            partition_dir (Path): The directory containing data for each\n",
        "                client/client id\n",
        "            model_generator (Callable[[], Module]): The model generator function\n",
        "        \"\"\"\n",
        "        self.cid = cid\n",
        "        log(INFO, \"cid: %s\", self.cid)\n",
        "        self.partition_dir = partition_dir\n",
        "        self.device = str(\n",
        "            torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        )\n",
        "        self.model_generator: Callable[[], Module] = model_generator\n",
        "        self.properties: dict[str, Scalar] = {\n",
        "            \"tensor_type\": \"numpy.ndarray\",\n",
        "            \"partition\": self.partition_dir,\n",
        "            \"cid\": self.cid\n",
        "            }\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def set_parameters(self, parameters: NDArrays) -> Module:\n",
        "        \"\"\"Load weights inside the network.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            parameters (NDArrays): set of weights to be loaded.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            [Module]: Network with new set of weights.\n",
        "        \"\"\"\n",
        "        net = self.model_generator()\n",
        "        return set_model_parameters(net, parameters)\n",
        "\n",
        "    def get_parameters(self, config: dict[str, Scalar]) -> NDArrays:\n",
        "        \"\"\"Return weights from a given model.\n",
        "\n",
        "        If no model is passed, then a local model is created.\n",
        "        This can be used to initialise a model in the\n",
        "        server.\n",
        "        The config param is not used but is mandatory in Flower.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            config (dict[int, Scalar]): dictionary containing configuration info.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            NDArrays: weights from the model.\n",
        "        \"\"\"\n",
        "        net = self.model_generator()\n",
        "        return get_model_parameters(net)\n",
        "\n",
        "    def fit(\n",
        "        self, parameters: NDArrays, config: dict[str, Scalar]\n",
        "    ) -> tuple[NDArrays, int, dict]:\n",
        "        \"\"\"Receive and train a model on the local client data.\n",
        "\n",
        "        It uses parameters from the config dict\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            net (NDArrays): Pytorch model parameters\n",
        "            config (dict[str, Scalar]): dictionary describing the training parameters\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            tuple[NDArrays, int, dict]: Returns the updated model, the size of the local\n",
        "                dataset and other metrics\n",
        "        \"\"\"\n",
        "        # Only create model right before training/testing\n",
        "        # To lower memory usage when idle\n",
        "        net = self.set_parameters(parameters)\n",
        "        net.to(self.device)\n",
        "\n",
        "        train_loader: DataLoader = self._create_data_loader(config, name=\"train\")\n",
        "        train_loss = self._train(net, train_loader=train_loader, config=config)\n",
        "        return get_model_parameters(net), len(train_loader), {\"train_loss\": train_loss}\n",
        "\n",
        "    def evaluate(\n",
        "        self, parameters: NDArrays, config: dict[str, Scalar]\n",
        "    ) -> tuple[float, int, dict]:\n",
        "        \"\"\"Receive and test a model on the local client data.\n",
        "\n",
        "        It uses parameters from the config dict\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            net (NDArrays): Pytorch model parameters\n",
        "            config (dict[str, Scalar]): dictionary describing the testing parameters\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            tuple[float, int, dict]: Returns the loss accumulate during testing, the\n",
        "                size of the local dataset and other metrics such as accuracy\n",
        "        \"\"\"\n",
        "        net = self.set_parameters(parameters)\n",
        "        net.to(self.device)\n",
        "\n",
        "        test_loader: DataLoader = self._create_data_loader(config, name=\"test\")\n",
        "        loss, accuracy = self._test(net, test_loader=test_loader, config=config)\n",
        "        return loss, len(test_loader), {\"local_accuracy\": accuracy}\n",
        "\n",
        "    def _create_data_loader(self, config: dict[str, Scalar], name: str) -> DataLoader:\n",
        "        \"\"\"Create the data loader using the specified config parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            config (dict[str, Scalar]): dictionary containing dataloader and dataset\n",
        "                parameters\n",
        "            mode (str): Load the training or testing set for the client\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            DataLoader: A pytorch dataloader iterable for training/testing\n",
        "        \"\"\"\n",
        "        batch_size = int(config[\"batch_size\"])\n",
        "        num_workers = int(config[\"num_workers\"])\n",
        "        dataset = self._load_dataset(name)\n",
        "        return DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=(name == \"train\"),\n",
        "        )\n",
        "\n",
        "    def _load_dataset(self, name: str) -> Dataset:\n",
        "        full_file: Path = self.partition_dir / str(self.cid)\n",
        "        return load_femnist_dataset(\n",
        "            mapping=full_file,\n",
        "            name=name,\n",
        "            data_dir=data_dir,\n",
        "        )\n",
        "\n",
        "    def _train(\n",
        "        self, net: Module, train_loader: DataLoader, config: dict[str, Scalar]\n",
        "    ) -> float:\n",
        "        return train_femnist(\n",
        "            net=net,\n",
        "            train_loader=train_loader,\n",
        "            epochs=int(config[\"epochs\"]),\n",
        "            device=self.device,\n",
        "            optimizer=torch.optim.AdamW(\n",
        "                net.parameters(),\n",
        "                lr=float(config[\"client_learning_rate\"]),\n",
        "                weight_decay=float(config[\"weight_decay\"]),\n",
        "            ),\n",
        "            criterion=torch.nn.CrossEntropyLoss(),\n",
        "            max_batches=int(config[\"max_batches\"]),\n",
        "        )\n",
        "\n",
        "    def _test(\n",
        "        self, net: Module, test_loader: DataLoader, config: dict[str, Scalar]\n",
        "    ) -> tuple[float, float]:\n",
        "        return test_femnist(\n",
        "            net=net,\n",
        "            test_loader=test_loader,\n",
        "            device=self.device,\n",
        "            criterion=torch.nn.CrossEntropyLoss(),\n",
        "            max_batches=int(config[\"max_batches\"]),\n",
        "        )\n",
        "\n",
        "    def get_properties(self, config: dict[str, Scalar]) -> dict[str, Scalar]:\n",
        "        \"\"\"Return properties for this client.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            config (dict[str, Scalar]): Options to be used for selecting specific\n",
        "            properties.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            dict[str, Scalar]: Returned properties.\n",
        "        \"\"\"\n",
        "        return self.properties\n",
        "\n",
        "    def get_train_set_size(self) -> int:\n",
        "        \"\"\"Return the client train set size.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            int: train set size of the client.\n",
        "        \"\"\"\n",
        "        return len(self._load_dataset(\"train\"))  # type: ignore[reportArgumentType]\n",
        "\n",
        "    def get_test_set_size(self) -> int:\n",
        "        \"\"\"Return the client test set size.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            int: test set size of the client.\n",
        "        \"\"\"\n",
        "        return len(self._load_dataset(\"test\"))  # type: ignore[reportArgumentType]\n",
        "\n",
        "\n",
        "# def fit_client_seeded(\n",
        "#     client: FlowerRayClient,\n",
        "#     params: NDArrays,\n",
        "#     conf: dict[str, Any],\n",
        "#     seed: Seeds = Seeds.DEFAULT,\n",
        "#     **kwargs: Any,\n",
        "# ) -> tuple[NDArrays, int, dict]:\n",
        "#     \"\"\"Wrap to always seed client training.\"\"\"\n",
        "#     np.random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     random.seed(seed)\n",
        "#     return client.fit(params, conf, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mwfh5JZmUcfd"
      },
      "outputs": [],
      "source": [
        "def get_flower_client_generator(\n",
        "    model_generator: Callable[[], Module],\n",
        "    partition_dir: Path,\n",
        "    mapping_fn: Callable[[int], int] | None = None,\n",
        ") -> Callable[[str], FlowerRayClient]:\n",
        "    \"\"\"Wrap the client instance generator.\n",
        "\n",
        "    This provides the client generator with a model generator function.\n",
        "    Also, the partition directory must be passed.\n",
        "    A mapping function could be used for filtering/ordering clients.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        model_generator (Callable[[], Module]): model generator function.\n",
        "        partition_dir (Path): directory containing the partition.\n",
        "        mapping_fn (Optional[Callable[[int], int]]): function mapping sorted/filtered\n",
        "            ids to real cid.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Callable[[str], FlowerRayClient]: client instance.\n",
        "    \"\"\"\n",
        "\n",
        "    def client_fn(cid: str) -> FlowerRayClient:\n",
        "        \"\"\"Create a single client instance given the client id `cid`.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            cid (str): client id, Flower requires this to be of type str.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            FlowerRayClient: client instance.\n",
        "        \"\"\"\n",
        "        return FlowerRayClient(\n",
        "            cid=mapping_fn(int(cid)) if mapping_fn is not None else int(cid),\n",
        "            partition_dir=partition_dir,\n",
        "            model_generator=model_generator,\n",
        "        )\n",
        "\n",
        "    return client_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udMWNQ0xU8VD",
        "outputId": "9228feef-c8dd-4577-fb2b-e656ef7688e0"
      },
      "outputs": [],
      "source": [
        "network_generator = get_network_generator()\n",
        "seed_net: Net = network_generator()\n",
        "seed_model_params: NDArrays = get_model_parameters(seed_net)\n",
        "\n",
        "centralized_flower_client_generator: Callable[[str], FlowerRayClient] = (\n",
        "    get_flower_client_generator(network_generator, centralized_partition)\n",
        ")\n",
        "centralized_flower_client = centralized_flower_client_generator(str(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_net.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJima3hoFBUD",
        "outputId": "71c5016c-ca5b-498d-de5a-dbc5ab8439e4"
      },
      "outputs": [],
      "source": [
        "# centralized_train_config: dict[str, Any] = {\n",
        "#     \"epochs\": 1,\n",
        "#     \"batch_size\": 32,\n",
        "#     \"client_learning_rate\": 0.01,\n",
        "#     \"weight_decay\": 0.001,\n",
        "#     \"num_workers\": 0,\n",
        "#     \"max_batches\": 100,\n",
        "# }\n",
        "\n",
        "# test_config: dict[str, Any] = {\n",
        "#     \"batch_size\": 32,\n",
        "#     \"num_workers\": 0,\n",
        "#     \"max_batches\": 100,\n",
        "# }\n",
        "\n",
        "# Train parameters on the centralised dataset\n",
        "# trained_params, num_examples, train_metrics = fit_client_seeded(\n",
        "#     centralized_flower_client, params=seed_model_params, conf=centralized_train_config\n",
        "# )\n",
        "# log(INFO, \"Train Metrics = %s\", train_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6QQv15XxXhvJ"
      },
      "outputs": [],
      "source": [
        "def sample_random_clients(\n",
        "    total_clients: int,\n",
        "    filter_less: int,\n",
        "    partition: Path,\n",
        "    seed: int | None = Seeds.DEFAULT,\n",
        ") -> Sequence[int]:\n",
        "    \"\"\"Sample randomly clients.\n",
        "\n",
        "    A filter on the client train set size is performed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        total_clients (int): total number of clients to sample.\n",
        "        filter_less (int): max number of train samples for which the client is\n",
        "            **discarded**.\n",
        "        partition (Path): path to the folder containing the partitioning.\n",
        "        seed (Optional[int], optional): seed for the random generator. Defaults to None.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Sequence[int]: list of sample client ids as int.\n",
        "    \"\"\"\n",
        "    real_federated_cid_client_generator: Callable[[str], FlowerRayClient] = (\n",
        "        get_flower_client_generator(network_generator, federated_partition)\n",
        "    )\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "    list_of_ids = []\n",
        "    while len(list_of_ids) < total_clients:\n",
        "        current_id = random.randint(0, 3229)\n",
        "        if (\n",
        "            real_federated_cid_client_generator(str(current_id)).get_train_set_size()\n",
        "            > filter_less\n",
        "        ):\n",
        "            list_of_ids.append(current_id)\n",
        "    return list_of_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "togXum333D1L"
      },
      "source": [
        "While FEMNIST has more than 3000 clients, our small-scale experiments will not require more than 100 at any point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-s7oVYNXkwf",
        "outputId": "0535dead-7487-4737-cea9-7b0b78184a5a"
      },
      "outputs": [],
      "source": [
        "total_clients: int = 100\n",
        "list_of_ids = sample_random_clients(\n",
        "    total_clients, 32, federated_partition\n",
        ")\n",
        "\n",
        "federated_client_generator: Callable[[str], FlowerRayClient] = (\n",
        "    get_flower_client_generator(\n",
        "        network_generator, federated_partition, lambda seq_id: list_of_ids[seq_id]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUrsR0JJwZ5R"
      },
      "source": [
        "Now, to test that the newly partitioned clients can be trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1HzR2ZY7nhPm"
      },
      "outputs": [],
      "source": [
        "test_config: dict[str, Any] = {\n",
        "    \"batch_size\": 32,\n",
        "    \"num_workers\": 0,\n",
        "    \"max_batches\": 100,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyoErIaM3D1L",
        "outputId": "9fa8cae1-66d7-4bc3-a9de-22282d5a8f14"
      },
      "outputs": [],
      "source": [
        "num_clients = 4\n",
        "clientIds = random.sample(list(range(total_clients)), num_clients)\n",
        "clients = [federated_client_generator(str(cid)) for cid in clientIds]\n",
        "print(f'{clients=}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6JaVhQIyL_Tq"
      },
      "outputs": [],
      "source": [
        "# def train(clients, numEpoch):\n",
        "#     epoch_config: dict[str, Any] = {\n",
        "#     \"epochs\": numEpoch,\n",
        "#     \"batch_size\": 32,\n",
        "#     \"client_learning_rate\": 0.01,\n",
        "#     \"weight_decay\": 0.001,\n",
        "#     \"num_workers\": 0,\n",
        "#     \"max_batches\": 100,\n",
        "#     }\n",
        "\n",
        "#     trained_models = [\n",
        "#     fit_client_seeded(\n",
        "#         client, seed_model_params, epoch_config\n",
        "#     )\n",
        "#     for client in clients\n",
        "#     ]\n",
        "\n",
        "#     params = [model for model, *rest in trained_models]\n",
        "#     metrics = [rest for _, *rest in trained_models]\n",
        "#     log(INFO, \"Metrics from trained models are: %s\", metrics)\n",
        "#     return params, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OO7CqzaE3D1M"
      },
      "outputs": [],
      "source": [
        "def get_federated_evaluation_function(\n",
        "    batch_size: int,\n",
        "    num_workers: int,\n",
        "    model_generator: Callable[[], Module],\n",
        "    criterion: Module,\n",
        "    max_batches: int,\n",
        ") -> Callable[[int, NDArrays, dict[str, Any]], tuple[float, dict[str, Scalar]]]:\n",
        "    \"\"\"Wrap the external federated evaluation function.\n",
        "\n",
        "    It provides the external federated evaluation function with some\n",
        "    parameters for the dataloader, the model generator function, and\n",
        "    the criterion used in the evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        batch_size (int): batch size of the test set to use.\n",
        "        num_workers (int): correspond to `num_workers` param in the Dataloader object.\n",
        "        model_generator (Callable[[], Module]):  model generator function.\n",
        "        criterion (Module): PyTorch Module containing the criterion for evaluating the\n",
        "        model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Callable[[int, NDArrays, dict[str, Any]], tuple[float, dict[str, Scalar]]]:\n",
        "            external federated evaluation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def federated_evaluation_function(\n",
        "        server_round: int,\n",
        "        parameters: NDArrays,\n",
        "        fed_eval_config: dict[\n",
        "            str, Any\n",
        "        ],  # mandatory argument, even if it's not being used\n",
        "    ) -> tuple[float, dict[str, Scalar]]:\n",
        "        \"\"\"Evaluate federated model on the server.\n",
        "\n",
        "        It uses the centralized val set for sake of simplicity.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            server_round (int): current federated round.\n",
        "            parameters (NDArrays): current model parameters.\n",
        "            fed_eval_config (dict[str, Any]): mandatory argument in Flower, can contain\n",
        "                some configuration info\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            tuple[float, dict[str, Scalar]]: evaluation results\n",
        "        \"\"\"\n",
        "        device: str = get_device()\n",
        "        net: Module = set_model_parameters(model_generator(), parameters)\n",
        "        net.to(device)\n",
        "\n",
        "        full_file: Path = centralized_mapping\n",
        "        dataset: Dataset = load_femnist_dataset(data_dir, full_file, \"val\")\n",
        "\n",
        "        valid_loader = DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        loss, acc = test_femnist(\n",
        "            net=net,\n",
        "            test_loader=valid_loader,\n",
        "            device=device,\n",
        "            criterion=criterion,\n",
        "            max_batches=max_batches,\n",
        "        )\n",
        "        return loss, {\"accuracy\": acc}\n",
        "\n",
        "    return federated_evaluation_function\n",
        "\n",
        "\n",
        "federated_evaluation_function = get_federated_evaluation_function(\n",
        "    batch_size=test_config[\"batch_size\"],\n",
        "    num_workers=test_config[\"num_workers\"],\n",
        "    model_generator=network_generator,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    max_batches=test_config[\"max_batches\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7D1cEkrB2B5r"
      },
      "outputs": [],
      "source": [
        "def aggregate_weighted_average(metrics: list[tuple[int, dict]]) -> dict:\n",
        "    \"\"\"Combine results from multiple clients following training or evaluation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        metrics (list[tuple[int, dict]]): collected clients metrics\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        dict: result dictionary containing the aggregate of the metrics passed.\n",
        "    \"\"\"\n",
        "    average_dict: dict = defaultdict(list)\n",
        "    total_examples: int = 0\n",
        "    for num_examples, metrics_dict in metrics:\n",
        "        for key, val in metrics_dict.items():\n",
        "            if isinstance(val, numbers.Number):\n",
        "                average_dict[key].append((num_examples, val))\n",
        "        total_examples += num_examples\n",
        "    return {\n",
        "        key: {\n",
        "            \"avg\": float(\n",
        "                sum([num_examples * metric for num_examples, metric in val])\n",
        "                / float(total_examples)\n",
        "            ),\n",
        "            \"all\": val,\n",
        "        }\n",
        "        for key, val in average_dict.items()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-sxyPBQ0zSoD"
      },
      "outputs": [],
      "source": [
        "# Federated configuration dictionary\n",
        "federated_train_config: dict[str, Any] = {\n",
        "    \"epochs\": 30,\n",
        "    \"batch_size\": 32,\n",
        "    \"client_learning_rate\": 0.01,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"num_workers\": 0,\n",
        "    \"max_batches\": 100,\n",
        "    \"central_dir\": home_dir / \"data\" / \"femnist\" / \"client_data_mappings\" / \"centralized\" / \"0\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNLIfqk0xhDj"
      },
      "source": [
        "The only challenge left is the FL simulation itself. In `Flower`, a `Server` object handles this for us by using `Ray` and spawning many heavyweight worker process.\n",
        "\n",
        "Given the limited-resource scenario in which we find ourselves, we provide you with a slightly modified simulation function which uses a simple thread pool. Feel free to swap it out for the original simulation or replace it with your own implementation if so inclined.\n",
        "\n",
        "> The server we use is not the default `Flower` server as it returns the model parameters from every single round in a `(round, NDArrays)` tuple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1Fs5gtK-3D1M"
      },
      "outputs": [],
      "source": [
        "def start_seeded_simulation(\n",
        "    client_fn: Callable[[str], Client],\n",
        "    num_clients: int,\n",
        "    config: ServerConfig,\n",
        "    strategy: Strategy,\n",
        "    name: str,\n",
        "    return_all_parameters: bool = False,\n",
        "    seed: int = Seeds.DEFAULT,\n",
        "    iteration: int = 0,\n",
        ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
        "    \"\"\"Wrap to seed client selection.\"\"\"\n",
        "    np.random.seed(seed ^ iteration)\n",
        "    torch.manual_seed(seed ^ iteration)\n",
        "    random.seed(seed ^ iteration)\n",
        "    parameter_list, hist = flwr.simulation.start_simulation_no_ray(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=num_clients,\n",
        "        client_resources={},\n",
        "        config=config,\n",
        "        strategy=strategy,\n",
        "    )\n",
        "    save_history(home_dir, hist, name)\n",
        "    return parameter_list, hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`run_simulation_frank_wolfe` is an adaptation of the original simulation function (now renamed to `run_simulation_fedavg`), the only difference being the strategy used. The strategy can be found in [c2m3/match/fed_frank_wolfe_strategy.py](https://github.com/DawnSpider96/L361-Federated-Learning/blob/c2m3/c2m3/match/fed_frank_wolfe_strategy.py#L43)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JkqplxMXLm8K"
      },
      "outputs": [],
      "source": [
        "num_rounds = 10\n",
        "\n",
        "num_total_clients = 20\n",
        "\n",
        "num_evaluate_clients = 0\n",
        "num_clients_per_round = 5\n",
        "\n",
        "initial_parameters = ndarrays_to_parameters(seed_model_params)\n",
        "\n",
        "\n",
        "def run_simulation_frank_wolfe(\n",
        "    # How long the FL process runs for:\n",
        "    num_rounds: int = num_rounds,\n",
        "    # Number of clients available\n",
        "    num_total_clients: int = num_total_clients,\n",
        "    # Number of clients used for train/eval\n",
        "    num_clients_per_round: int = num_clients_per_round,\n",
        "    num_evaluate_clients: int = num_evaluate_clients,\n",
        "    # If less clients are overall available stop FL\n",
        "    min_available_clients: int = num_total_clients,\n",
        "    # If less clients are available for fit/eval stop FL\n",
        "    min_fit_clients: int = num_clients_per_round,\n",
        "    min_evaluate_clients: int = num_evaluate_clients,\n",
        "    # Function to test the federated model performance\n",
        "    # external to a client instantiation\n",
        "    evaluate_fn: (\n",
        "        Callable[\n",
        "            [int, NDArrays, dict[str, Scalar]],\n",
        "            tuple[float, dict[str, Scalar]] | None,\n",
        "        ]\n",
        "        | None\n",
        "    ) = federated_evaluation_function,\n",
        "    # Functions to generate a config for client fit/evaluate\n",
        "    # by-default the same config is shallow-copied to all clients in Flower\n",
        "    # this version simply uses the configs defined above\n",
        "    on_fit_config_fn: Callable[\n",
        "        [int], dict[str, Scalar]\n",
        "    ] = lambda _x: federated_train_config,\n",
        "    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] = lambda _x: test_config,\n",
        "    # The \"Parameters\" type is merely a more packed version\n",
        "    # of numpy array lists, used internally by Flower\n",
        "    initial_parameters: Parameters = initial_parameters,\n",
        "    # If this is set to True, aggregation will work even if some clients fail\n",
        "    accept_failures: bool = False,\n",
        "    # How to combine the metrics dictionary returned by all clients for fit/eval\n",
        "    fit_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
        "    evaluate_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
        "    federated_client_generator: Callable[\n",
        "        [str], flwr.client.NumPyClient\n",
        "    ] = federated_client_generator,\n",
        "    # Aggregation learning rate for FedAvg\n",
        "    server_learning_rate: float = 1.0,\n",
        "    server_momentum: float = 0.0,\n",
        ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
        "    \"\"\"Run a federated simulation using Flower.\"\"\"\n",
        "    log(INFO, \"FL will execute for %s rounds\", num_rounds)\n",
        "\n",
        "    # Percentage of clients used for train/eval\n",
        "    fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
        "    fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
        "\n",
        "    strategy = FrankWolfeSync(\n",
        "        fraction_fit=fraction_fit,\n",
        "        fraction_evaluate=fraction_evaluate,\n",
        "        min_fit_clients=min_fit_clients,\n",
        "        min_evaluate_clients=min_evaluate_clients,\n",
        "        min_available_clients=min_available_clients,\n",
        "        on_fit_config_fn=on_fit_config_fn,\n",
        "        on_evaluate_config_fn=on_evaluate_config_fn,\n",
        "        evaluate_fn=evaluate_fn,\n",
        "        initial_parameters=initial_parameters,\n",
        "        accept_failures=accept_failures,\n",
        "        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
        "        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        "        # batch_size = int(config[\"batch_size\"])\n",
        "        # num_workers = int(config[\"num_workers\"])\n",
        "        # dataset = self._load_dataset(name)\n",
        "    )\n",
        "    # resetting the seed for the random selection of clients\n",
        "    # this way the list of clients trained is guaranteed to be always the same\n",
        "\n",
        "    cfg = ServerConfig(num_rounds)\n",
        "\n",
        "    def simulator_client_generator(cid: str) -> Client:\n",
        "        return federated_client_generator(cid).to_client()\n",
        "\n",
        "    parameters_for_each_round, hist = start_seeded_simulation(\n",
        "        client_fn=simulator_client_generator,\n",
        "        num_clients=num_total_clients,\n",
        "        config=cfg,\n",
        "        strategy=strategy,\n",
        "        name=\"c2m3\",\n",
        "        return_all_parameters=True,\n",
        "        seed=Seeds.DEFAULT,\n",
        "    )\n",
        "    return parameters_for_each_round, hist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_simulation_fedavg(\n",
        "    # How long the FL process runs for:\n",
        "    num_rounds: int = num_rounds,\n",
        "    # Number of clients available\n",
        "    num_total_clients: int = num_total_clients,\n",
        "    # Number of clients used for train/eval\n",
        "    num_clients_per_round: int = num_clients_per_round,\n",
        "    num_evaluate_clients: int = num_evaluate_clients,\n",
        "    # If less clients are overall available stop FL\n",
        "    min_available_clients: int = num_total_clients,\n",
        "    # If less clients are available for fit/eval stop FL\n",
        "    min_fit_clients: int = num_clients_per_round,\n",
        "    min_evaluate_clients: int = num_evaluate_clients,\n",
        "    # Function to test the federated model performance\n",
        "    # external to a client instantiation\n",
        "    evaluate_fn: (\n",
        "        Callable[\n",
        "            [int, NDArrays, dict[str, Scalar]],\n",
        "            tuple[float, dict[str, Scalar]] | None,\n",
        "        ]\n",
        "        | None\n",
        "    ) = federated_evaluation_function,\n",
        "    # Functions to generate a config for client fit/evaluate\n",
        "    # by-default the same config is shallow-copied to all clients in Flower\n",
        "    # this version simply uses the configs defined above\n",
        "    on_fit_config_fn: Callable[\n",
        "        [int], dict[str, Scalar]\n",
        "    ] = lambda _x: federated_train_config,\n",
        "    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] = lambda _x: test_config,\n",
        "    # The \"Parameters\" type is merely a more packed version\n",
        "    # of numpy array lists, used internally by Flower\n",
        "    initial_parameters: Parameters = initial_parameters,\n",
        "    # If this is set to True, aggregation will work even if some clients fail\n",
        "    accept_failures: bool = False,\n",
        "    # How to combine the metrics dictionary returned by all clients for fit/eval\n",
        "    fit_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
        "    evaluate_metrics_aggregation_fn: Callable | None = aggregate_weighted_average,\n",
        "    federated_client_generator: Callable[\n",
        "        [str], flwr.client.NumPyClient\n",
        "    ] = federated_client_generator,\n",
        "    # Aggregation learning rate for FedAvg\n",
        "    server_learning_rate: float = 1.0,\n",
        "    server_momentum: float = 0.0,\n",
        ") -> tuple[list[tuple[int, NDArrays]], History]:\n",
        "    \"\"\"Run a federated simulation using Flower.\"\"\"\n",
        "    log(INFO, \"FL will execute for %s rounds\", num_rounds)\n",
        "\n",
        "    # Percentage of clients used for train/eval\n",
        "    fraction_fit: float = float(num_clients_per_round) / num_total_clients\n",
        "    fraction_evaluate: float = float(num_evaluate_clients) / num_total_clients\n",
        "\n",
        "    strategy = FedAvg(\n",
        "        fraction_fit=fraction_fit,\n",
        "        fraction_evaluate=fraction_evaluate,\n",
        "        min_fit_clients=min_fit_clients,\n",
        "        min_evaluate_clients=min_evaluate_clients,\n",
        "        min_available_clients=min_available_clients,\n",
        "        on_fit_config_fn=on_fit_config_fn,\n",
        "        on_evaluate_config_fn=on_evaluate_config_fn,\n",
        "        evaluate_fn=evaluate_fn,\n",
        "        initial_parameters=initial_parameters,\n",
        "        accept_failures=accept_failures,\n",
        "        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
        "        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        "        server_learning_rate=server_learning_rate,\n",
        "        server_momentum=server_momentum,\n",
        "        # batch_size = int(config[\"batch_size\"])\n",
        "        # num_workers = int(config[\"num_workers\"])\n",
        "        # dataset = self._load_dataset(name)\n",
        "        \n",
        "    )\n",
        "    # resetting the seed for the random selection of clients\n",
        "    # this way the list of clients trained is guaranteed to be always the same\n",
        "\n",
        "    cfg = ServerConfig(num_rounds)\n",
        "\n",
        "    def simulator_client_generator(cid: str) -> Client:\n",
        "        return federated_client_generator(cid).to_client()\n",
        "\n",
        "    parameters_for_each_round, hist = start_seeded_simulation(\n",
        "        client_fn=simulator_client_generator,\n",
        "        num_clients=num_total_clients,\n",
        "        config=cfg,\n",
        "        strategy=strategy,\n",
        "        name=\"c2m3\",\n",
        "        return_all_parameters=True,\n",
        "        seed=Seeds.DEFAULT,\n",
        "    )\n",
        "    return parameters_for_each_round, hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See below: This is a copy of one of the gradient matrices, output of [collect_gradients_frank_wolfe_model_pair](https://github.com/crisostomi/cycle-consistent-model-merging/blob/6ee822f56114181ea7eba4cb7533a0b6e27ea749/src/ccmm/matching/frank_wolfe_sync_matching.py#L72). It (like the other gradient matrices) is too skewed towards the central diagonal (ie the value of assigning the i-th worker to the i-th task is significantly higher than any other task).\n",
        "\n",
        "Hence the output of the `linear_sum_assignment` will specify no permutations whatsoever. This effect carries over into the updating of permutation matrices, where the projected gradients will be updated to identity matrices, and afterwards the updated permutation matrices do not change from how they were initialised (identity matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "cost = np.array([[ 5.6573396 ,  0.12816703,  0.7734268 ,  0.54617214, -0.10447261,\n",
        "         0.39014438],\n",
        "         [ 1.1931067 ,  1.5022745 ,  7.1273413 ,  0.79405797,  0.34921485,\n",
        "         1.2170054 ],\n",
        "       [ 0.29189748,  5.714079  ,  1.1553278 ,  0.15853024,  0.47528026,\n",
        "         0.7882026 ],\n",
        "       [ 0.7397236 ,  0.10359962,  0.6959048 ,  5.283332  ,  0.27297193,\n",
        "         0.99973345],\n",
        "       [ 0.5185135 ,  1.0186552 ,  1.0369794 ,  0.6470743 ,  5.6826034 ,\n",
        "         1.0205868 ],\n",
        "       [ 0.8446162 ,  0.9361492 ,  1.0751884 ,  1.6859208 ,  0.61274135,\n",
        "         6.6700554 ]])\n",
        "linear_sum_assignment(cost, maximize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HpswKXbDup",
        "outputId": "d6f104af-93b4-47fe-cea8-06fe21dd5ab4"
      },
      "outputs": [],
      "source": [
        "parameters_for_each_round, hist = run_simulation_frank_wolfe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters_for_each_round_fedavg, hist_fedavg = run_simulation_fedavg()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist_fedavg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "save_dir = Path.cwd() / \"../../fed_results\"\n",
        "save_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Convert the history objects to dictionaries\n",
        "hist_dict = {\n",
        "    \"metrics_centralized\": hist.metrics_centralized,\n",
        "    \"losses_centralized\": hist.losses_centralized,\n",
        "    \"metrics_distributed\": hist.metrics_distributed,\n",
        "    \"losses_distributed\": hist.losses_distributed\n",
        "}\n",
        "\n",
        "hist_fedavg_dict = {\n",
        "    \"metrics_centralized\": hist_fedavg.metrics_centralized,\n",
        "    \"losses_centralized\": hist_fedavg.losses_centralized,\n",
        "    \"metrics_distributed\": hist_fedavg.metrics_distributed,\n",
        "    \"losses_distributed\": hist_fedavg.losses_distributed\n",
        "}\n",
        "\n",
        "parameters_list = [param.tolist() if isinstance(param, np.ndarray) else param \n",
        "                  for param in parameters_for_each_round]\n",
        "\n",
        "parameters_fedavg_list = [param.tolist() if isinstance(param, np.ndarray) else param \n",
        "                         for param in parameters_for_each_round_fedavg]\n",
        "\n",
        "results_data = {\n",
        "    \"c2m3\": {\n",
        "        \"parameters\": parameters_list,\n",
        "        \"history\": hist_dict\n",
        "    },\n",
        "    \"fedavg\": {\n",
        "        \"parameters\": parameters_fedavg_list,\n",
        "        \"history\": hist_fedavg_dict\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to a JSON file\n",
        "with open(save_dir / f\"flwr_{Seeds.DEFAULT}\", \"w\") as f:\n",
        "    json.dump(results_data, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {save_dir / 'flower_simulation_results.json'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def plot_metrics(hist1, hist2, legend_labels=['FrankWolfe', 'FedAvg'], save_path=None):\n",
        "    \n",
        "#     acc1 = hist1.metrics_centralized['accuracy']\n",
        "#     rounds_acc1, acc_values1 = zip(*acc1)\n",
        "    \n",
        "#     acc2 = hist2.metrics_centralized['accuracy']\n",
        "#     rounds_acc2, acc_values2 = zip(*acc2)\n",
        "    \n",
        "#     loss1 = hist1.losses_centralized\n",
        "#     rounds_loss1, loss_values1 = zip(*loss1)\n",
        "    \n",
        "#     loss2 = hist2.losses_centralized\n",
        "#     rounds_loss2, loss_values2 = zip(*loss2)\n",
        "    \n",
        "#     fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    \n",
        "#     axs[0].plot(rounds_acc1, acc_values1, 'o-', color='blue', linewidth=2, markersize=8, \n",
        "#                label=f'{legend_labels[0]} Accuracy')\n",
        "#     axs[0].plot(rounds_acc2, acc_values2, 's-', color='cyan', linewidth=2, markersize=8, \n",
        "#                label=f'{legend_labels[1]} Accuracy')\n",
        "    \n",
        "#     axs[0].set_title('Accuracy Comparison', fontsize=14)\n",
        "#     axs[0].set_xlabel('Round Number', fontsize=12)\n",
        "#     axs[0].set_ylabel('Accuracy', fontsize=12)\n",
        "#     axs[0].grid(True, linestyle='--', alpha=0.7)\n",
        "#     axs[0].legend(loc='best')\n",
        "    \n",
        "#     all_rounds_acc = sorted(list(set(rounds_acc1 + rounds_acc2)))\n",
        "#     axs[0].set_xticks(all_rounds_acc)\n",
        "    \n",
        "#     axs[1].plot(rounds_loss1, loss_values1, 'o-', color='red', linewidth=2, markersize=8, \n",
        "#                label=f'{legend_labels[0]} Loss')\n",
        "#     axs[1].plot(rounds_loss2, loss_values2, 's-', color='orange', linewidth=2, markersize=8, \n",
        "#                label=f'{legend_labels[1]} Loss')\n",
        "    \n",
        "#     axs[1].set_title('Loss Comparison', fontsize=14)\n",
        "#     axs[1].set_xlabel('Round Number', fontsize=12)\n",
        "#     axs[1].set_ylabel('Loss', fontsize=12)\n",
        "#     axs[1].grid(True, linestyle='--', alpha=0.7)\n",
        "#     axs[1].legend(loc='best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot_metrics(hist, hist_fedavg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CMXI3Uz3D1M",
        "outputId": "78b78966-9748-424a-94ed-c005ee70e844"
      },
      "outputs": [],
      "source": [
        "# log(\n",
        "#     INFO,\n",
        "#     \"Size of the list with the model parameters: %s\",\n",
        "#     len(parameters_for_each_round),\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fed",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
